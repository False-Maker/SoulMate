# 后端核心链路深度优化方案

结合整个项目代码的深度扫描，在前端页面之外，发现以下 3 个关键的后端/逻辑层优化点，能够显著提升应用的**响应速度**、**稳定性**和**智能程度**。

## 1. 体验质变：LLM 真流式改造 (Real Streaming)
**现状**: 目前的 `chatStreamWithMessages` 是**“伪流式”**。它实际上是等待 LLM 生成完所有文本（阻塞），然后在本地模拟打字机效果。
*   **问题**: 用户必须等待完整回复生成后才能看到第一个字。对于长回复，这可能导致 5-10 秒的空白等待，体验极差。
*   **优化方案**:
    *   改造 `LLMApiService`，支持 Retrofit 的 `@Streaming`。
    *   在 `LLMService` 中实现 SSE (Server-Sent Events) 解析器，实时处理 `data: ` 数据块。
    *   **预期效果**: 首字延迟 (TTFT) 降低至 1秒以内，实现“边想边说”的即时反馈。

## 2. 稳定性增强：API 调用重试机制
**现状**: `LLMService` 和 `EmbeddingService` 均缺乏重试逻辑。
*   **问题**: 移动端网络环境复杂，任何一次网络抖动都会导致对话直接失败，显示错误信息。
*   **优化方案**:
    *   引入**指数退避 (Exponential Backoff)** 重试策略。
    *   当遇到 5xx 服务器错误或网络超时时，自动重试 3 次（间隔 1s, 2s, 4s）。
    *   **预期效果**: 大幅减少因网络波动导致的“对话失败”提示。

## 3. 数据质量风控：移除 Embedding 随机降级
**现状**: `DoubaoEmbeddingService` 在 API 调用失败或无 Key 时，会生成**随机向量**。
*   **问题**: 这些随机向量如果被存入数据库，会导致记忆检索完全失效（检索到无关记忆），且难以清洗（脏数据污染）。
*   **优化方案**:
    *   移除 `generateMockEmbedding` 降级逻辑。
    *   失败时明确抛出异常，上层业务捕获后可选择降级为“仅保存文本不存向量”或提示用户重试。
    *   **预期效果**: 保证记忆库中 100% 为高质量语义向量，维护 RAG 检索的准确性。

## 执行计划
1.  **改造 LLM 接口**: 修改 `LLMApiService` 增加流式接口。
2.  **实现 SSE 解析**: 在 `LLMService` 中编写流式处理逻辑。
3.  **增加重试拦截**: 使用 Kotlin 协程实现通用的 `retry` 包装器。
4.  **清洗降级逻辑**: 删除 Embedding 服务的随机生成代码。

这些优化不涉及前端 UI 调整，但能从根本上提升 App 的“内功”。